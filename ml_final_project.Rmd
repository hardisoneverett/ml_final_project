---
title: "final_project"
author: "Hardison Everett"
date: "3/29/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, cache.lazy = FALSE,
                      warning = FALSE, message = FALSE)
```

For this project we were asked to build a machine learning algorithm to classify how well 6 participants performed an exercise activity.  The data to be evaluated was generated by accelerometers placed on different body parts of the participants. 

To begin with, we will read in the data using the code below.

```{r read_data}
library(readr)

model_data <- read.csv(file = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                     header = TRUE, sep = ",")
validation <- read.csv(file = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                    header = TRUE, sep = ",")
```

The `model_data` will be used to build and initially test the algorith.  The `validation` data is the data we are asked to predict for the project.

After initially viewing `model_data` and `validation`, it becomes apparent many of the columns consist primarily of NAs.  The code below checks each columns percent NA and sorts descending. The `validation` data has 100 colummns consisting entirely of NAs while the `model_data` has 67 columns with over 97% NAs.  We then check if the list of NA columns from the `validation` set contains all of the primarily NA columns from both sets, which it does contain.  The NA columns can now be filtered out using `emptycol_val`.
``` {r NAs, results=FALSE}
## Checking columns for NAs
        ##model_data
sort(colSums(is.na(model_data))/nrow(model_data), decreasing = TRUE)
        ##validation
sort(colSums(is.na(validation))/nrow(validation), decreasing = TRUE)

## Creating variable to sort NAs from the model_data
emptycol_mod<-colSums(is.na(model_data))/nrow(model_data) > 0.9

## Creating variable to sort NAs from the validation data
emptycol_val<-colSums(is.na(validation)) == nrow(validation)

## Checking if emptycol_val contains all NA columns
length(emptycol_mod[emptycol_mod] %in% emptycol_val[emptycol_val])
```

The code below filters `model_data` using `emptycol_val`.  Additionally, the first seven columns are not included in the model since these columns contain time and user information.
```{r clean_data}
train_data <- (model_data[!emptycol_val])
train_data <- train_data[,-c(1:7)]
```

Next, the `train_data` columns will be checked for variance.
```{r near_zero_values, results=FALSE}
library(caret)
nearZeroVar(train_data)
```

The remaining columns appear to have at least some variance.  At this point we could use exploratory analysis to determine feature/variable importance, however, since we have no expert knowledge, we are just going to pass all variables to the model.  Once we build the model, we will be able to view variable importance based on our model.  
        Next we move on to partitioning the `train_data`.  The `train_data` will be partitioned into`training` and `testing` sets, with 75% going into the `training` set and the remaining 25% to `testing`.  We also change the `classe` variable to type factor. Additionally, we create the folds for cross validation.  In this case, we will just be using 5 folds. 
```{r partition_data}
library(tidymodels)

set.seed(1234)
model_split <- initial_split(train_data)
training <- training(model_split)
training$classe<-as.factor(training$classe)
testing <- testing(model_split)

set.seed(1234)
training_folds<-vfold_cv(training, v = 5)
```

Now, we can begin to build the model.  The code below will provide us with a template of the code needed to build the model.  In this case, we will be using a ranger implementation of random forest.  We're using ranger because it seems to run fairly quickly and efficiently.
```{r training, results=FALSE}
library(usemodels)
library(ranger)
use_ranger(classe ~ . ,training)
```
Most of the code below will be auto generated by the previous code.  However, we need to change `resamples` in `tune_grid` to our cross-validation folds `training_folds`.  We also set `grid` to 10.  This will try 10 combinations of tuning parameters which will later be used to tune the model. 
```{r }
ranger_recipe <- 
  recipe(formula = classe ~ ., data = training) 

ranger_spec <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_mode("classification") %>% 
  set_engine("ranger") 

ranger_workflow <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(ranger_spec) 

set.seed(20829)
ranger_tune <-
  tune_grid(ranger_workflow, 
            resamples = training_folds, 
            grid = 10)
```

Next, we can examine the results of the models generated.  The code below will show the 5 best performing models based on accuracy.  Additionally, the `autoplot` will show `mtry` and `min_n` versus `accuracy` and `roc_auc`.  Examining these two outputs, we can see that all combinations perform well.  With that being said, a `mtry` (# Randomly Selected Predictors) of 22 and `min_n` (Minimal Node Size) of 4 seem to perform the best.  These values will be used in the final model.     
```{r results}
show_best(ranger_tune, metric ="accuracy", n = 5 )
autoplot(ranger_tune)
```

The code below finalizes the model using the parameters from the best model.
```{r finalize model, results=FALSE}
final_model <- ranger_workflow%>%
                        finalize_workflow(select_best(ranger_tune))
final_model
```
Next, we will fit the final model to all of the training data and evaluate on the testing data.
```{r final fit, results=FALSE}

final_fit<-last_fit(final_model, model_split)
final_fit
```
Now we can review the results.  Here we can see the accuracy and roc_auc for the `testing` data.  The accuracy on the `testing` set is 0.994, which is higher than any of the initial models tested while tuning parameters. It should also be noted the roc_auc is rounded up to 1. 
```{r metrics}
collect_metrics(final_fit)
```
We can also view a confusion matrix for the results. 
```{r test predictions}
library(ggplot2)
collect_predictions(final_fit)%>%
        conf_mat(truth=classe, estimate = .pred_class)

```
Finally, we can predict on the `validation` set.  The model correctly predicts all 20 observations in the `validation` set.
```{r predicting validation, results=FALSE}
predict(final_fit$.workflow[[1]], validation)

```

The code below will show a plot of the importance of the different variables.  We could use this to try to reduce the number of variables used in a modified model.
```{r vip}
library(vip)
imp <- ranger_spec %>%
        finalize_model(select_best(ranger_tune, metric ="roc_auc"))%>%
        set_engine("ranger", importance = "permutation")

workflow()%>%
        add_recipe(ranger_recipe)%>%
        add_model(imp)%>%
        fit(training)%>%
        pull_workflow_fit()%>%
        vip(num_features = 20)
```
